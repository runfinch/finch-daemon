name: samcli-test-parallel

on:
  push:
    branches:
      - main
    paths:
      - '**.go'
      - 'go.mod'
      - 'go.sum'
      - '.github/workflows/samcli-test-parallel.yaml'
      - 'Makefile*'
      - 'cmd/**'
      - 'pkg/**'
      - 'internal/**'
      - 'api/**'
  pull_request:
    branches:
      - main
    paths:
      - '**.go'
      - 'go.mod'
      - 'go.sum'
      - '.github/workflows/samcli-test-parallel.yaml'
      - 'Makefile*'
      - 'cmd/**'
      - 'pkg/**'
      - 'internal/**'
      - 'api/**'
  # schedule:
  #   - cron: '0 9 * * *' # midnight
  workflow_dispatch:    # manual trigger

env:
  GO_VERSION: '1.23.8'
  GOLANGCI_LINT_VERSION: '1.64.8'
  CONTAINERD_VERSION: "1.7.27" # picking one version; 2.1.3 validated through ci.yaml

permissions:
  # This is required for configure-aws-credentials to request an OIDC JWT ID token to access AWS resources later on.
  # More info: https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/about-security-hardening-with-openid-connect#adding-permissions-settings
  id-token: write
  contents: read    # This is required for actions/checkout

jobs:
  samcli-test:
    runs-on: ubuntu-latest
    timeout-minutes: 70 # allows 30+ min buffer (decrease for parallel)
    strategy:
      fail-fast: false  # Don't cancel other jobs if one fails
      matrix:
        test-suite: 
          # - unit-tests
          - invoke
          - start-lambda
          - start-api
          - sync
          - package
          - deploy
          - build
    env:
      AWS_DEFAULT_REGION: ${{ secrets.REGION }}
      BY_CANARY: true
      DOCKER_HOST: unix:///run/finch.sock
      DOCKER_CONFIG: /home/runner/.docker # finch-daemon supports Docker API
      SAM_CLI_DEV: 1 
      UNIQUE_TEST_ID: ${{ github.run_id }}-${{ matrix.test-suite }}-${{ strategy.job-index }}
    steps:

      # Setup steps inspired by e2e-test in ci.yaml

      - name: Set up Go
        uses: actions/setup-go@d35c59abb061a4a6fb18e82ac0862c26744d6ab5 # v5.5.0
        with:
          go-version: ${{ env.GO_VERSION }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11.12' # aws-sam-cli/setup.py -> python_requires=">=3.9, <=4.0, !=4.0" -> actually, align wuth doc

      - name: Checkout finch-daemon repo
        uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332 # v4.1.7

      - name: Setup Docker config for finch-daemon
        run: |
          mkdir -p $DOCKER_CONFIG
          echo '{"auths":{"docker.io":{},"public.ecr.aws":{}}}' > $DOCKER_CONFIG/config.json
          echo "Docker config created at: $DOCKER_CONFIG"
          echo "Docker host set to: $DOCKER_HOST"

        # Version verification (from Q)
      - name: Verify required tools (strict)
        run: |
          echo "=== Strict Tool Version Verification ==="

          # Check GNU Make version (>= 3.81)
          echo "Checking make version..."
          make --version | head -1
          if ! make --version | grep -q "GNU Make"; then
            echo "ERROR: GNU Make not found"
            exit 1
          fi
          if ! make --version | grep -qE "GNU Make [4-9]\.|GNU Make 3\.[8-9]"; then
            echo "ERROR: GNU Make version must be >= 3.81"
            exit 1
          fi
          echo "Make version OK"

          # Check AWS CLI version (>= 2.15.22)
          echo "Checking AWS CLI version..."
          aws --version
          if ! aws --version 2>&1 | grep -q "aws-cli/2\."; then
            echo "ERROR: AWS CLI v2 not found"
            exit 1
          fi
          # Extract version and check if >= 2.15.22
          AWS_VERSION=$(aws --version 2>&1 | grep -oE "aws-cli/2\.[0-9]+\.[0-9]+" | cut -d'/' -f2)
          if ! echo "$AWS_VERSION" | awk -F. '{if($1>=2 && $2>=15 && $3>=22) exit 0; else exit 1}'; then
            echo "ERROR: AWS CLI version must be >= 2.15.22, found: $AWS_VERSION"
            exit 1
          fi
          echo "AWS CLI version OK"

          # Check Go version (>= 1.23.0)
          echo "Checking Go version..."
          go version
          if ! go version | grep -q "go version go"; then
            echo "ERROR: Go not found"
            exit 1
          fi
          # Extract version and check if >= 1.23.0
          GO_VERSION=$(go version | grep -oE "go[0-9]+\.[0-9]+\.[0-9]+" | cut -c3-)
          if ! echo "$GO_VERSION" | awk -F. '{if($1>=1 && $2>=23) exit 0; else exit 1}'; then
            echo "ERROR: Go version must be >= 1.23.0, found: $GO_VERSION"
            exit 1
          fi
          echo "Go version OK"

          # Check Python version (>= 3.9.0)
          echo "Checking Python version..."
          python --version
          if ! python --version | grep -q "Python 3\."; then
            echo "ERROR: Python 3 not found"
            exit 1
          fi
          # Extract version and check if >= 3.9.0
          PYTHON_VERSION=$(python --version | grep -oE "Python 3\.[0-9]+\.[0-9]+" | cut -d' ' -f2)
          if ! echo "$PYTHON_VERSION" | awk -F. '{if($1>=3 && $2>=9) exit 0; else exit 1}'; then
            echo "ERROR: Python version must be >= 3.9.0, found: $PYTHON_VERSION"
            exit 1
          fi
          echo "Python version OK"

          echo "=== All tool versions verified successfully ==="

      - name: Stop pre-existing containerd and docker services
        run: |
          sudo systemctl stop docker
          sudo systemctl stop containerd

      - name: Install finch-daemon dependencies
        run: ./setup-test-env.sh

      - name: Build finch-daemon
        run: make build

      - name: Remove default podman network config
        run: |
          sudo ls /etc/cni/net.d
          sudo rm /etc/cni/net.d/87-podman-bridge.conflist

      - name: Verify CNI plugins installed
        run: |
          echo "=== CNI Plugin Verification ==="
          echo "Checking critical CNI plugins..."
          ls -la /opt/cni/bin/bridge || { echo "❌ Bridge plugin missing"; exit 1; }
          ls -la /opt/cni/bin/loopback || { echo "❌ Loopback plugin missing"; exit 1; }
          ls -la /opt/cni/bin/host-local || { echo "❌ Host-local plugin missing"; exit 1; }
          echo "✅ All critical CNI plugins present"
          
          echo "All CNI plugins:"
          ls -la /opt/cni/bin/

      - name: Clean up Daemon socket
        run: |
          sudo rm -f /run/finch.sock
          sudo rm -f /run/finch.pid
          sudo rm -f /run/finch-credential.sock

      - name: Start finch-daemon
        run: |
          sudo cp bin/docker-credential-finch /usr/bin
          sudo bin/finch-daemon --debug --socket-owner $UID &
          sleep 10  # Conservative wait for daemon startup
          curl --unix-socket /run/finch.sock http://localhost/_ping
          echo "finch-daemon is ready"

      - name: Verify and configure Docker environment
        run: |
          echo "DOCKER_HOST: $DOCKER_HOST"
          echo "DOCKER_CONFIG: $DOCKER_CONFIG"
          ls -la $DOCKER_CONFIG/
          curl --unix-socket ${DOCKER_HOST#unix://} http://localhost/_ping

          # Use the default builder and set it to use docker driver
          docker buildx use default
          docker buildx inspect --bootstrap 

          # Test that builds actually work through finch-daemon
          echo "FROM alpine:latest" | docker buildx build --load -t test-build -
          docker rmi test-build

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@b47578312673ae6fa5b5096b330d9fbac3d116df # v4.2.1
        with:
          role-to-assume: ${{ secrets.ROLE }}
          role-session-name: samcli-integration-test-${{ matrix.test-suite }}
          aws-region: ${{ secrets.REGION }}

      - name: Set up SAM CLI from source
        run: |
          git clone https://github.com/aws/aws-sam-cli.git
          cd aws-sam-cli
          git checkout v1.142.1
          git submodule update --init --recursive
          python -m pip install --upgrade pip
          make init
          which samdev 
          samdev --version

      # TESTING STARTS HERE - Matrix-based test execution

      - name: Run ${{ matrix.test-suite }} tests
        working-directory: aws-sam-cli
        run: |
          echo "=== Running ${{ matrix.test-suite }} test suite ==="
          case "${{ matrix.test-suite }}" in
            invoke)
              # Run the test with enhanced error capture
              echo "=== Running Invoke Tests ==="
              export SAM_CLI_STACK_NAME="invoke-${UNIQUE_TEST_ID}"
              timeout 75m python -m pytest tests/integration/local/invoke -k 'not Terraform' -v -s --tb=long > test_output.txt 2>&1 || true
              
              # Show test output
              cat test_output.txt
              
              # Extract and analyze specific build errors
              echo "=== Analyzing Build Errors ==="
              grep -A 5 -B 5 "Failed to build Docker Image" test_output.txt || echo "No 'Failed to build Docker Image' errors found"
              grep -A 5 -B 5 "NoneType: None" test_output.txt || echo "No 'NoneType: None' errors found"
              grep -A 5 -B 5 "no image was built" test_output.txt || echo "No 'no image was built' errors found"
              
              # Check for any Docker-related errors in the test output
              echo "=== Docker-related Errors in Tests ==="
              grep -i "docker" test_output.txt | grep -i "error\|fail\|exception" || echo "No Docker errors found in test output"

              # Create a list of expected failing tests
              cat > expected_failures.txt << 'EOF'
          test_invoke_with_error_during_image_build
          test_invoke_with_timeout_set_0_TimeoutFunction
          test_invoke_with_timeout_set_1_TimeoutFunctionWithParameter
          test_invoke_with_timeout_set_2_TimeoutFunctionWithStringParameter
          test_building_new_rapid_image_removes_old_rapid_images
          # test_invoke_returns_expected_results_from_git_function
          # test_invoke_returns_expected_results_from_git_function_with_parameters
          EOF

              # Extract actual failing tests
              grep "FAILED" test_output.txt | sed 's/.*::\(test_[a-zA-Z0-9_]*\).*/\1/' > actual_failures.txt

              # Find unexpected failures (failures not in the expected list)
              UNEXPECTED_FAILURES=$(grep -v -f expected_failures.txt actual_failures.txt || true)

              # Check if there are any unexpected failures
              if [ -n "$UNEXPECTED_FAILURES" ]; then
                echo "Unexpected test failures found:"
                echo "$UNEXPECTED_FAILURES"
                echo "Invoke test failed due to unexpected test failures"
                exit 1
              else
                echo "All test failures were expected. Invoke test passed!"
              fi
              ;;
              
            start-lambda)
              echo "=== Running start-lambda tests ==="
              export SAM_CLI_STACK_NAME="start-lambda-${UNIQUE_TEST_ID}"
              timeout 30m  python -m pytest tests/integration/local/start_lambda -k 'not Terraform' -v > start_lambda_output.txt 2>&1 || true
              cat start_lambda_output.txt

              # Check if any tests failed
              if grep -q "FAILED" start_lambda_output.txt; then
                echo "Some start-lambda tests failed!"
                exit 1
              else
                echo "All start-lambda tests passed!"
              fi
              ;;
              
            start-api)
              echo "=== Running start-api tests ==="
              export SAM_CLI_STACK_NAME="start-api-${UNIQUE_TEST_ID}"
              # Increase file limit to prevent "too many files open" error
              ulimit -n 8192

              timeout 75m  python -m pytest tests/integration/local/start_api -k 'not Terraform' -v > start_api_output.txt 2>&1 || true
              cat start_api_output.txt

              # Check if any tests failed
              if grep -q "FAILED" start_api_output.txt; then
                echo "Some start-api tests failed!"
                exit 1
              else
                echo "All start-api tests passed!"
              fi
              ;;
              
            sync)
              echo "=== Running sync tests ==="
              export SAM_CLI_STACK_NAME="sync-${UNIQUE_TEST_ID}"
              timeout 20m  python -m pytest tests/integration/sync -k 'image' -v > sync_test.txt 2>&1 || true
              cat sync_test.txt

              # Check if any tests failed
              if grep -q "FAILED" sync_test.txt; then
                echo "Some sync tests failed!"
                exit 1
              else
                echo "All sync tests passed!"
              fi
              ;;
              
            package)
              echo "=== Running package tests ==="
              export SAM_CLI_STACK_NAME="package-${UNIQUE_TEST_ID}"
              timeout 7m  python -m pytest tests/integration/package/test_package_command_image.py > package_test.txt 2>&1 || true
              cat package_test.txt

              # Create a list of expected failing tests
              cat > expected_package_failures.txt << 'EOF'
          test_package_with_deep_nested_template_image
          test_package_template_with_image_repositories_nested_stack
          test_package_with_loadable_image_archive_0_template_image_load_yaml
          EOF

              # Extract actual failing tests
              grep "FAILED" package_test.txt | sed 's/.*::\(test_[a-zA-Z0-9_]*\).*/\1/' > actual_package_failures.txt || true

              # Find unexpected failures (failures not in the expected list)
              UNEXPECTED_FAILURES=$(grep -v -f expected_package_failures.txt actual_package_failures.txt || true)

              # Check if there are any unexpected failures
              if [ -n "$UNEXPECTED_FAILURES" ]; then
                echo "Unexpected test failures found:"
                echo "$UNEXPECTED_FAILURES"
                echo "Package test failed due to unexpected test failures"
                exit 1
              else
                echo "All test failures were expected. Test passed!"
              fi
              ;;
              
            deploy)
              echo "=== Running deploy tests ==="
              export SAM_CLI_STACK_NAME="deploy-${UNIQUE_TEST_ID}"
              timeout 30m  python -m pytest tests/integration/deploy -k 'image' > deploy_test.txt 2>&1 || true
              cat deploy_test.txt

              # Check if any tests failed
              if grep -q "PASSED" deploy_test.txt; then
                echo "Some deploy tests passed unexpectedly! This might indicate a change in behavior."
                grep "PASSED" deploy_test.txt
                exit 1
              else
                echo "All deploy tests failed as expected!"
              fi

              # Show which specific tests passed to understand if they should pass
              echo "=== Deploy Test Results ==="
              grep -E "(PASSED|FAILED|ERROR)" deploy_test.txt
              echo "=== End Results ==="
              ;;
                            
            build)
              echo "=== Running build tests ==="
              export SAM_CLI_STACK_NAME="build-${UNIQUE_TEST_ID}"
              python -m pytest tests/integration/buildcmd -k '(container or image) and not sar and not terraform' > build_test.txt 2>&1 || true
              cat build_test.txt

              # Create a list of expected failing tests
              cat > expected_build_failures.txt << 'EOF'
          test_with_invalid_dockerfile_definition
          test_with_invalid_dockerfile_location
          test_load_success
          test_building_ruby_3_2_1_use_container
          test_with_makefile_builder_specified_python_runtime_1_use_container
          test_with_native_builder_specified_python_runtime_1_use_container
          test_inline_not_built_1_use_container
          test_json_env_vars_passed_0_use_container
          test_json_env_vars_passed_1_use_container
          test_inline_env_vars_passed_0_use_container
          test_inline_env_vars_passed_1_use_container
          test_custom_build_image_succeeds_0_use_container
          test_custom_build_image_succeeds_1_use_container
          test_building_ruby_in_container_with_specified_architecture_0_ruby3_2
          test_building_java_in_container_with_arm64_architecture_00_java8_al2
          test_building_java_in_container_with_arm64_architecture_03_java8_al2
          test_building_java_in_container_with_arm64_architecture_04_java11
          test_building_java_in_container_with_arm64_architecture_07_java11
          test_building_java_in_container_with_arm64_architecture_08_java17
          test_building_java_in_container_with_arm64_architecture_11_java17
          test_building_java_in_container_with_arm64_architecture_al2023_0_java21
          test_building_java_in_container_with_arm64_architecture_al2023_1_java21
          test_building_java_in_container_with_arm64_architecture_al2023_2_java21
          test_building_java_in_container_with_arm64_architecture_al2023_3_java21
          test_building_java_in_container_00_java8_al2
          EOF

              # Extract actual failing tests
              grep "FAILED" build_test.txt | sed 's/.*::\(test_[a-zA-Z0-9_]*\).*/\1/' > actual_build_failures.txt

              # Find unexpected failures (failures not in the expected list)
              UNEXPECTED_FAILURES=$(grep -v -f expected_build_failures.txt actual_build_failures.txt || true)

              # Check if there are any unexpected failures
              if [ -n "$UNEXPECTED_FAILURES" ]; then
                echo "Unexpected test failures found:"
                echo "$UNEXPECTED_FAILURES"
                echo "Build test failed due to unexpected test failures"
                exit 1
              else
                echo "All test failures were expected. Test passed!"
              fi
              ;;
              
            *)
              echo "Unknown test suite: ${{ matrix.test-suite }}"
              exit 1
              ;;
          esac

      - name: Cleanup CloudFormation stacks
        if: always()  # Run even if tests failed
        run: |
          echo "=== Cleaning up CloudFormation stacks for ${{ matrix.test-suite }} ==="
          # Clean up any stacks that might have been created
          aws cloudformation delete-stack --stack-name "sync-${UNIQUE_TEST_ID}" || true
          aws cloudformation delete-stack --stack-name "package-${UNIQUE_TEST_ID}" || true
          aws cloudformation delete-stack --stack-name "deploy-${UNIQUE_TEST_ID}" || true
          echo "Cleanup commands issued (stacks will delete asynchronously)"

      - name: Capture finch-daemon logs
        if: always()  # Run even if previous steps failed
        run: |
          echo "=== Finch-Daemon System Logs for ${{ matrix.test-suite }} ==="
          sudo journalctl --since "60 minutes ago" | grep -i finch || echo "No finch logs found"
          
          echo "=== Containerd Logs ==="
          sudo journalctl --since "60 minutes ago" | grep -i containerd | tail -50 || echo "No containerd logs found"
          
          echo "=== Process Status ==="
          ps aux | grep -E "(finch-daemon|containerd)" | grep -v grep || echo "No relevant processes found"
