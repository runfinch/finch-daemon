name: samcli-test

on:
  push:
    branches:
      - main
    paths:
      - '**.go'
      - 'go.mod'
      - 'go.sum'
      - '.github/workflows/samcli-test.yaml'
      - 'Makefile*'
      - 'cmd/**'
      - 'pkg/**'
      - 'internal/**'
      - 'api/**'
  pull_request:
    branches:
      - main
    paths:
      - '**.go'
      - 'go.mod'
      - 'go.sum'
      - '.github/workflows/samcli-test.yaml'
      - 'Makefile*'
      - 'cmd/**'
      - 'pkg/**'
      - 'internal/**'
      - 'api/**'
  # schedule:
  #   - cron: '0 9 * * *' # midnight
  workflow_dispatch:    # manual trigger
env:
  GO_VERSION: '1.23.8'
  GOLANGCI_LINT_VERSION: '1.64.8'
  CONTAINERD_VERSION: "1.7.27" # picking one version; 2.1.3 validated through ci.yaml

permissions:
  # This is required for configure-aws-credentials to request an OIDC JWT ID token to access AWS resources later on.
  # More info: https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/about-security-hardening-with-openid-connect#adding-permissions-settings
  id-token: write
  contents: read    # This is required for actions/checkout
jobs:
  samcli-test:
    runs-on: ubuntu-latest
    timeout-minutes: 250 # allows 30+ min buffer
    env:
      AWS_DEFAULT_REGION: ${{ secrets.REGION }}
      BY_CANARY: true
      DOCKER_HOST: unix:///run/finch.sock
      DOCKER_CONFIG: /home/runner/.docker # finch-daemon supports Docker API
    steps:

      # Setup steps inspired by e2e-test in ci.yaml

      - name: Set up Go
        uses: actions/setup-go@d35c59abb061a4a6fb18e82ac0862c26744d6ab5 # v5.5.0
        with:
          go-version: ${{ env.GO_VERSION }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11.12' # aws-sam-cli/setup.py -> python_requires=">=3.9, <=4.0, !=4.0" -> actually, align wuth doc

      - name: Checkout finch-daemon repo
        uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332 # v4.1.7

      - name: Setup Docker config for finch-daemon
        run: |
          mkdir -p $DOCKER_CONFIG
          echo '{"auths":{"docker.io":{},"public.ecr.aws":{}}}' > $DOCKER_CONFIG/config.json
          echo "Docker config created at: $DOCKER_CONFIG"
          echo "Docker host set to: $DOCKER_HOST"

        # Version verification (from Q)
      - name: Verify required tools (strict)
        run: |
          echo "=== Strict Tool Version Verification ==="

          # Check GNU Make version (>= 3.81)
          echo "Checking make version..."
          make --version | head -1
          if ! make --version | grep -q "GNU Make"; then
            echo "ERROR: GNU Make not found"
            exit 1
          fi
          if ! make --version | grep -qE "GNU Make [4-9]\.|GNU Make 3\.[8-9]"; then
            echo "ERROR: GNU Make version must be >= 3.81"
            exit 1
          fi
          echo "Make version OK"

          # Check AWS CLI version (>= 2.15.22)
          echo "Checking AWS CLI version..."
          aws --version
          if ! aws --version 2>&1 | grep -q "aws-cli/2\."; then
            echo "ERROR: AWS CLI v2 not found"
            exit 1
          fi
          # Extract version and check if >= 2.15.22
          AWS_VERSION=$(aws --version 2>&1 | grep -oE "aws-cli/2\.[0-9]+\.[0-9]+" | cut -d'/' -f2)
          if ! echo "$AWS_VERSION" | awk -F. '{if($1>=2 && $2>=15 && $3>=22) exit 0; else exit 1}'; then
            echo "ERROR: AWS CLI version must be >= 2.15.22, found: $AWS_VERSION"
            exit 1
          fi
          echo "AWS CLI version OK"

          # Check Go version (>= 1.23.0)
          echo "Checking Go version..."
          go version
          if ! go version | grep -q "go version go"; then
            echo "ERROR: Go not found"
            exit 1
          fi
          # Extract version and check if >= 1.23.0
          GO_VERSION=$(go version | grep -oE "go[0-9]+\.[0-9]+\.[0-9]+" | cut -c3-)
          if ! echo "$GO_VERSION" | awk -F. '{if($1>=1 && $2>=23) exit 0; else exit 1}'; then
            echo "ERROR: Go version must be >= 1.23.0, found: $GO_VERSION"
            exit 1
          fi
          echo "Go version OK"

          # Check Python version (>= 3.9.0)
          echo "Checking Python version..."
          python --version
          if ! python --version | grep -q "Python 3\."; then
            echo "ERROR: Python 3 not found"
            exit 1
          fi
          # Extract version and check if >= 3.9.0
          PYTHON_VERSION=$(python --version | grep -oE "Python 3\.[0-9]+\.[0-9]+" | cut -d' ' -f2)
          if ! echo "$PYTHON_VERSION" | awk -F. '{if($1>=3 && $2>=9) exit 0; else exit 1}'; then
            echo "ERROR: Python version must be >= 3.9.0, found: $PYTHON_VERSION"
            exit 1
          fi
          echo "Python version OK"

          echo "=== All tool versions verified successfully ==="

      - name: Stop pre-existing containerd and docker services
        run: |
          sudo systemctl stop docker
          sudo systemctl stop containerd

      - name: Install finch-daemon dependencies
        run: ./setup-test-env.sh

      - name: Build finch-daemon
        run: make build

      - name: Remove default podman network config
        run: |
          sudo ls /etc/cni/net.d
          sudo rm /etc/cni/net.d/87-podman-bridge.conflist

      - name: Clean up Daemon socket
        run: |
          sudo rm -f /run/finch.sock
          sudo rm -f /run/finch.pid
          sudo rm -f /run/finch-credential.sock

      - name: Start finch-daemon
        run: |
          sudo cp bin/docker-credential-finch /usr/bin
          sudo bin/finch-daemon --debug --socket-owner $UID &
          sleep 10  # Conservative wait for daemon startup
          curl --unix-socket /run/finch.sock http://localhost/_ping
          echo "finch-daemon is ready"

      - name: Verify Docker environment
        run: |
          echo "DOCKER_HOST: $DOCKER_HOST"
          echo "DOCKER_CONFIG: $DOCKER_CONFIG"
          ls -la $DOCKER_CONFIG/
          curl --unix-socket ${DOCKER_HOST#unix://} http://localhost/_ping
          
          # Configure buildx to connect to existing buildkit daemon
          docker buildx create --name finch-builder --driver remote unix:///run/buildkit/buildkitd.sock --use

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@b47578312673ae6fa5b5096b330d9fbac3d116df # v4.2.1
        with:
          role-to-assume: ${{ secrets.ROLE }}
          role-session-name: samcli-integration-test
          aws-region: ${{ secrets.REGION }}

      - name: Set up SAM CLI from source
        run: |
          git clone git@github.com:aws/aws-sam-cli.git
          cd aws-sam-cli
          git checkout $(git tag --sort=-version:refname | grep -E '^v[0-9]+\.[0-9]+\.[0-9]+$' | head -n1)
          git submodule update --init --recursive
          python -m pip install --upgrade pip
          make init
          which samdev
          samdev --version

      # TESTING STARTS HERE

      # - name: Run unit tests
      #   working-directory: aws-sam-cli
      #   run: |

      #     # Fix ulimit ahead of time (not unlimited)
      #     ulimit -n 8192

      #     # Clean up potentially problematic Docker config entries
      #     if [ -f ~/.docker/config.json ]; then
      #       if command -v jq >/dev/null 2>&1; then
      #         jq 'del(.credsStore)' ~/.docker/config.json > ~/.docker/config.json.tmp && mv ~/.docker/config.json.tmp ~/.docker/config.json
      #       else
      #         # Fallback: remove credsStore line with sed
      #         sed -i '/"credsStore"/d' ~/.docker/config.json
      #       fi
      #     fi

      #     # Run the unit tests and capture output
      #     make test | tee unit_test_output.txt

      #     # Extract coverage percentage
      #     COVERAGE=$(grep -o "[0-9]\+%" unit_test_output.txt | head -1 | tr -d '%')

      #     # Check if coverage is at least 90% (allowing for some variation from the ~94%)
      #     if [ -z "$COVERAGE" ] || [ "$COVERAGE" -lt 90 ]; then
      #       echo "Coverage is below expected threshold! Got: $COVERAGE%, Expected: ~94%"
      #       exit 1
      #     else
      #       echo "Unit tests passed with $COVERAGE% coverage (expected ~94%)"
      #     fi

      - name: Run local invoke test
        timeout-minutes: 75
        working-directory: aws-sam-cli
        run: |
          echo "=== Enhanced BuildKit Health Check ==="
          # Check if BuildKit daemon is running
          ps aux | grep buildkitd | grep -v grep || echo "❌ BuildKit daemon not running!"
          
          # Find ALL socket files on the system
          echo "=== Finding ALL BuildKit Sockets ==="
          find /run -name "*.sock" 2>/dev/null | grep -i buildkit || echo "No buildkit sockets in /run"
          find /var/lib/buildkit -name "*.sock" 2>/dev/null || echo "No buildkit sockets in /var/lib/buildkit"
          find /tmp -name "*buildkit*" 2>/dev/null || echo "No buildkit files in /tmp"
          
          # Check what's actually in the buildkit directory
          echo "=== Contents of /run/buildkit ==="
          ls -la /run/buildkit/ 2>/dev/null || echo "/run/buildkit directory empty or inaccessible"
          
          # Find what sockets the buildkitd process is actually using
          echo "=== BuildKit Process Socket Usage ==="
          BUILDKIT_PID=$(pgrep buildkitd)
          if [ -n "$BUILDKIT_PID" ]; then
            echo "BuildKit PID: $BUILDKIT_PID"
            sudo lsof -p $BUILDKIT_PID | grep sock || echo "No sockets found for buildkitd process"
          else
            echo "BuildKit process not found"
          fi
          
          # Try to find the actual socket location
          echo "=== Searching for BuildKit Socket Everywhere ==="
          find / -name "*buildkit*.sock" 2>/dev/null | head -5 || echo "No buildkit sockets found anywhere"
          
          # Fix BuildKit permissions based on discovery
          echo "=== Applying BuildKit Permission Fix ==="
          sudo chmod 755 /run/buildkit/ 2>/dev/null || echo "Could not fix directory permissions"
          sudo chmod 666 /run/buildkit/buildkitd.sock 2>/dev/null || echo "Could not fix socket permissions"
          
          # Test BuildKit connectivity (uses default socket)
          buildctl debug workers || echo "❌ BuildKit not responding!"
          
          # Test Docker daemon connectivity
          echo "=== Docker Daemon Check ==="
          docker version || echo "❌ Docker daemon not responding!"
          docker info | grep -E "(Server Version|Storage Driver|Logging Driver)" || echo "❌ Docker info failed!"
          
          # Test basic Docker build functionality
          echo "=== Docker Build Test ==="
          echo 'FROM alpine:latest' > /tmp/test.dockerfile
          echo 'RUN echo "test build"' >> /tmp/test.dockerfile
          
          # Verify buildx driver
          echo "Current buildx driver:"
          docker buildx ls
          
          # Build with explicit load to ensure image goes to finch-daemon
          docker buildx build -f /tmp/test.dockerfile -t test-build --load . || echo "❌ Basic Docker build failed!"
          docker images | grep test-build || echo "❌ Test image not found!"
          docker rmi test-build 2>/dev/null || echo "❌ Failed to remove test image"
          
          echo "=== End Enhanced Check ==="

          # Run the test with enhanced error capture
          echo "=== Running Invoke Tests with Enhanced Debugging ==="
          SAM_CLI_DEV=1 python -m pytest tests/integration/local/invoke -k 'not Terraform' -v -s --tb=long > test_output.txt 2>&1 || true
          
          # Show test output
          cat test_output.txt
          
          # Extract and analyze specific build errors
          echo "=== Analyzing Build Errors ==="
          grep -A 5 -B 5 "Failed to build Docker Image" test_output.txt || echo "No 'Failed to build Docker Image' errors found"
          grep -A 5 -B 5 "NoneType: None" test_output.txt || echo "No 'NoneType: None' errors found"
          grep -A 5 -B 5 "no image was built" test_output.txt || echo "No 'no image was built' errors found"
          
          # Check for any Docker-related errors in the test output
          echo "=== Docker-related Errors in Tests ==="
          grep -i "docker" test_output.txt | grep -i "error\|fail\|exception" || echo "No Docker errors found in test output"

          # Create a list of expected failing tests
          cat > expected_failures.txt << 'EOF'
          test_invoke_with_error_during_image_build
          test_invoke_with_timeout_set_0_TimeoutFunction
          test_invoke_with_timeout_set_1_TimeoutFunctionWithParameter
          test_invoke_with_timeout_set_2_TimeoutFunctionWithStringParameter
          test_building_new_rapid_image_removes_old_rapid_images
          # test_invoke_returns_expected_results_from_git_function
          # test_invoke_returns_expected_results_from_git_function_with_parameters
          EOF

          # Extract actual failing tests
          grep "FAILED" test_output.txt | sed 's/.*::\(test_[a-zA-Z0-9_]*\).*/\1/' > actual_failures.txt

          # Find unexpected failures (failures not in the expected list)
          UNEXPECTED_FAILURES=$(grep -v -f expected_failures.txt actual_failures.txt || true)

          # Check if there are any unexpected failures
          if [ -n "$UNEXPECTED_FAILURES" ]; then
            echo "Unexpected test failures found:"
            echo "$UNEXPECTED_FAILURES"
            echo "Invoke test failed due to unexpected test failures"
            exit 1
          else
            echo "All test failures were expected. Invoke test passed!"
          fi

          echo "=== Post-test BuildKit Health Check ==="
          ps aux | grep buildkitd | grep -v grep || echo "❌ BuildKit daemon not running!"
          buildctl debug workers || echo "❌ BuildKit not responding!"
          echo "=== End Post-test Check ==="


      - name: Run start-lambda test
        timeout-minutes: 30
        working-directory: aws-sam-cli
        run: |
          # Run the tests
          SAM_CLI_DEV=1 python -m pytest tests/integration/local/start_lambda -k 'not Terraform' -v > start_lambda_output.txt 2>&1 || true
          cat start_lambda_output.txt

          # Check if any tests failed
          if grep -q "FAILED" start_lambda_output.txt; then
            echo "Some start-lambda tests failed!"
            exit 1
          else
            echo "All start-lambda tests passed!"
          fi

      - name: Run start-api test
        timeout-minutes: 75
        working-directory: aws-sam-cli
        run: |
          # Increase file limit to prevent "too many files open" error
          ulimit -n 8192

          # Run the tests
          SAM_CLI_DEV=1 python -m pytest tests/integration/local/start_api -k 'not Terraform' -v > start_api_output.txt 2>&1 || true
          cat start_api_output.txt

          # Check if any tests failed
          if grep -q "FAILED" start_api_output.txt; then
            echo "Some start-api tests failed!"
            exit 1
          else
            echo "All start-api tests passed!"
          fi

      - name: Run sync test
        timeout-minutes: 20
        working-directory: aws-sam-cli
        run: |
          # Run the tests
          SAM_CLI_DEV=1 python -m pytest tests/integration/sync -k 'image' -v > sync_test.txt 2>&1 || true
          cat sync_test.txt

          # Check if any tests failed
          if grep -q "FAILED" sync_test.txt; then
            echo "Some sync tests failed!"
            exit 1
          else
            echo "All sync tests passed!"
          fi

      - name: Run package test
        timeout-minutes: 7
        working-directory: aws-sam-cli
        run: |
          # Run the tests
          SAM_CLI_DEV=1 python -m pytest tests/integration/package/test_package_command_image.py > package_test.txt 2>&1 || true
          cat package_test.txt

          # Create a list of expected failing tests
          cat > expected_package_failures.txt << 'EOF'
          test_package_with_deep_nested_template_image
          test_package_template_with_image_repositories_nested_stack
          test_package_with_loadable_image_archive_0_template_image_load_yaml
          EOF

          # Extract actual failing tests
          grep "FAILED" package_test.txt | sed 's/.*::\(test_[a-zA-Z0-9_]*\).*/\1/' > actual_package_failures.txt || true

          # Find unexpected failures (failures not in the expected list)
          UNEXPECTED_FAILURES=$(grep -v -f expected_package_failures.txt actual_package_failures.txt || true)

          # Check if there are any unexpected failures
          if [ -n "$UNEXPECTED_FAILURES" ]; then
            echo "Unexpected test failures found:"
            echo "$UNEXPECTED_FAILURES"
            echo "Package test failed due to unexpected test failures"
            exit 1
          else
            echo "All test failures were expected. Test passed!"
          fi

      - name: Run deploy test
        timeout-minutes: 30
        working-directory: aws-sam-cli
        run: |
          # Run the tests
          SAM_CLI_DEV=1 python -m pytest tests/integration/deploy -k 'image' > deploy_test.txt 2>&1 || true
          cat deploy_test.txt

          # Check if any tests failed
          if grep -q "PASSED" deploy_test.txt; then
            echo "Some deploy tests passed unexpectedly! This might indicate a change in behavior."
            grep "PASSED" deploy_test.txt
            exit 1
          else
            echo "All deploy tests failed as expected!"
          fi

      - name: Run build test
        working-directory: aws-sam-cli
        run: |
          # Run the tests
          SAM_CLI_DEV=1 python -m pytest tests/integration/buildcmd -k '(container or image) and not sar and not terraform' > build_test.txt 2>&1 || true
          cat build_test.txt

          # Create a list of expected failing tests
          cat > expected_build_failures.txt << 'EOF'
          test_with_invalid_dockerfile_definition
          test_with_invalid_dockerfile_location
          test_load_success
          test_building_ruby_3_2_1_use_container
          test_with_makefile_builder_specified_python_runtime_1_use_container
          test_with_native_builder_specified_python_runtime_1_use_container
          test_inline_not_built_1_use_container
          test_json_env_vars_passed_0_use_container
          test_json_env_vars_passed_1_use_container
          test_inline_env_vars_passed_0_use_container
          test_inline_env_vars_passed_1_use_container
          test_custom_build_image_succeeds_0_use_container
          test_custom_build_image_succeeds_1_use_container
          test_building_ruby_in_container_with_specified_architecture_0_ruby3_2
          test_building_java_in_container_with_arm64_architecture_00_java8_al2
          test_building_java_in_container_with_arm64_architecture_03_java8_al2
          test_building_java_in_container_with_arm64_architecture_04_java11
          test_building_java_in_container_with_arm64_architecture_07_java11
          test_building_java_in_container_with_arm64_architecture_08_java17
          test_building_java_in_container_with_arm64_architecture_11_java17
          test_building_java_in_container_with_arm64_architecture_al2023_0_java21
          test_building_java_in_container_with_arm64_architecture_al2023_1_java21
          test_building_java_in_container_with_arm64_architecture_al2023_2_java21
          test_building_java_in_container_with_arm64_architecture_al2023_3_java21
          test_building_java_in_container_00_java8_al2
          EOF

          # Extract actual failing tests
          grep "FAILED" build_test.txt | sed 's/.*::\(test_[a-zA-Z0-9_]*\).*/\1/' > actual_build_failures.txt

          # Find unexpected failures (failures not in the expected list)
          UNEXPECTED_FAILURES=$(grep -v -f expected_build_failures.txt actual_build_failures.txt || true)

          # Check if there are any unexpected failures
          if [ -n "$UNEXPECTED_FAILURES" ]; then
            echo "Unexpected test failures found:"
            echo "$UNEXPECTED_FAILURES"
            echo "Build test failed due to unexpected test failures"
            exit 1
          else
            echo "All test failures were expected. Test passed!"
          fi

      - name: Capture finch-daemon logs
        if: always()  # Run even if previous steps failed
        run: |
          echo "=== Finch-Daemon System Logs ==="
          sudo journalctl --since "60 minutes ago" | grep -i finch || echo "No finch logs found"
          
          echo "=== Containerd Logs ==="
          sudo journalctl --since "60 minutes ago" | grep -i containerd | tail -50 || echo "No containerd logs found"
          
          echo "=== BuildKit Logs ==="
          sudo journalctl --since "60 minutes ago" | grep -i buildkit | tail -50 || echo "No buildkit logs found"
          
          echo "=== Process Status ==="
          ps aux | grep -E "(finch-daemon|containerd|buildkit)" | grep -v grep || echo "No relevant processes found"
